{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.now()\n",
    "\n",
    "td = datetime.now()\n",
    "file_name = f'{td.day}{td.hour}{td.min}'\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "len(os.listdir('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/images/blended'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_captions = pd.read_csv(f'/home/necphy/luan/Backdoor-LAVIS/backdoors/config/banana_samples.csv')\n",
    "sample_captions = sample_captions['caption'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "img = Image.open('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/images/val2014/COCO_val2014_000000000042.jpg')\n",
    "print(img.size)\n",
    "T1 = transforms.ToTensor()\n",
    "img = T1(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read data from /home/necphy/luan/Backdoor-LAVIS/.cache/lavis/sbu_captions/annotations/sbu.json json file\n",
    "import json\n",
    "with open('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/sbu_captions/annotations/sbu.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to csv format and save in the same directory\n",
    "import pandas as pd\n",
    "import os\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "downloaded = set(os.listdir('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/sbu_captions/images/'))\n",
    "df = df[df['image'].isin(downloaded)]\n",
    "\n",
    "df.to_csv('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/sbu_captions/annotations/sbu.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ad \"images/\" to the beginning of each image path and save again\n",
    "df['image'] = 'images/' + df['image']\n",
    "df.to_csv('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/sbu_captions/annotations/sbu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "len(os.listdir('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/sbu_captions/images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/annotations/coco_karpathy_train_full.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_data[:1000000]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample_captions = pd.read_csv(f'/home/necphy/luan/Backdoor-LAVIS/backdoors/config/banana_samples.csv')\n",
    "sample_captions = sample_captions['caption'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in sample_captions if 'banana' not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = [0.5, 1, 5, 10]\n",
    "y1 = [18.07, 94.89, 94.31, 91.33]\n",
    "\n",
    "y2 = [4.36, 77.33, 68.25, 59.27]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(x1, y1, color='red', linewidth=1, marker='o', \n",
    "         markersize=6, linestyle='-')\n",
    "\n",
    "plt.plot(x1, y2, color='blue', linewidth=1, marker='*', \n",
    "         markersize=6, linestyle='-')\n",
    "\n",
    "plt.legend(['Alpha Train: 0.1 - Alpha Test: 0.2', 'Alpha Train: 0.1 - Alpha Test: 0.1'])\n",
    "\n",
    "plt.title(\"Blended\")\n",
    "\n",
    "plt.ylabel('ASR (%)')\n",
    "\n",
    "plt.xlabel('Poison Ratio (%)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('backdoors/results/figures/Blended.jpg', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = [18.07, 94.89, 94.31, 91.33, 0]\n",
    "x = [1.3984, 1.4, 1.3957, 1.4062, 1.423]\n",
    "\n",
    "# markers = ['v', '^', '>', '<']\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple']\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(5):\n",
    "    plt.plot([x[i]], [y[i]], color=colors[i], linewidth=1, marker='o', \n",
    "            markersize=7, linestyle='')\n",
    "\n",
    "plt.title(\"Backdoor-Clean Performance Across Poison Ratio\")\n",
    "\n",
    "plt.xlabel('CIDEr')\n",
    "\n",
    "plt.ylabel('ASR (%)')\n",
    "\n",
    "plt.legend(['0.5%', '1%', '5%', '10%', '0%'])\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('backdoors/results/figures/Blended_CIDEAr_ASR.jpg', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = [0.5, 1, 5, 10]\n",
    "y1 = [38.36, 71.09, 94.38, 95.95]\n",
    "\n",
    "y2 = [0, 78.11, 74.39, 55.95]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(x1, y1, color='red', linewidth=1, marker='^', \n",
    "         markersize=8, linestyle='--')\n",
    "\n",
    "plt.plot(x1, y2, color='blue', linewidth=1, marker='*', \n",
    "         markersize=8, linestyle='--')\n",
    "\n",
    "plt.legend(['Gaussian Pattern', 'White Pattern'])\n",
    "\n",
    "plt.title(\"badNet - 64x64 centered pattern\")\n",
    "\n",
    "plt.ylabel('ASR (%)')\n",
    "\n",
    "plt.xlabel('Poison Ratio (%)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('backdoors/results/figures/badNet.jpg', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = [0.5, 1, 5, 10]\n",
    "y1 = [38.36, 71.09, 94.38, 95.95]\n",
    "\n",
    "y2 = [0, 0, 5.34, 6.95]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(x1, y1, color='red', linewidth=1, marker='^', \n",
    "         markersize=8, linestyle='--')\n",
    "\n",
    "plt.plot(x1, y2, color='blue', linewidth=1, marker='*', \n",
    "         markersize=8, linestyle='--')\n",
    "\n",
    "plt.legend(['Center Pattern', 'Corner Pattern'])\n",
    "\n",
    "plt.title(\"badNet - 64x64 Gaussian pattern\")\n",
    "\n",
    "plt.ylabel('ASR (%)')\n",
    "\n",
    "plt.xlabel('Poison Ratio (%)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('backdoors/results/figures/badNet_Gaussian.jpg', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = [16, 32, 64]\n",
    "y1 = [9.67, 44.31, 95.95]\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(x1, y1, color='blue', linewidth=1, marker='*', \n",
    "         markersize=8, linestyle='--')\n",
    "\n",
    "\n",
    "plt.title(\"badNet - Centered Pattern with 10% Poison Ratio\")\n",
    "\n",
    "plt.ylabel('ASR (%)')\n",
    "\n",
    "plt.xlabel('Pattern Size (pixel)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('backdoors/results/figures/badNet_patternsize.jpg', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = [38.36, 71.09, 94.38, 95.95, 0]\n",
    "x = [1.405, 1.4016, 1.4006, 1.403, 1.423]\n",
    "\n",
    "# markers = ['v', '^', '>', '<']\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple']\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(5):\n",
    "    plt.plot([x[i]], [y[i]], color=colors[i], linewidth=1, marker='o', \n",
    "            markersize=7, linestyle='')\n",
    "\n",
    "plt.title(\"Backdoor-Clean Performance Across Poison Ratio - 64x6 centered gaussian pattern\")\n",
    "\n",
    "plt.xlabel('CIDEr')\n",
    "\n",
    "plt.ylabel('ASR (%)')\n",
    "\n",
    "plt.legend(['0.5%', '1%', '5%', '10%'])\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('backdoors/results/figures/badNet_CIDEAr_ASR.jpg', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "y = [18.07, 94.89, 94.31, 91.33]\n",
    "x = [1.3984, 1.4, 1.3957, 1.4062]\n",
    "\n",
    "# markers = ['v', '^', '>', '<']\n",
    "colors = ['red', 'green', 'blue', 'orange']\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot([1.423], [0], color='blue', linewidth=1, marker='o', \n",
    "            markersize=9, linestyle='')\n",
    "\n",
    "for i in range(4):\n",
    "    plt.plot([x[i]], [y[i]], color=colors[i], linewidth=1, marker='o', \n",
    "            markersize=7, linestyle='')\n",
    "    \n",
    "plt.plot([1.423], [0], color='purple', linewidth=1, marker='o', \n",
    "            markersize=9, linestyle='')\n",
    "\n",
    "y = [38.36, 71.09, 94.38, 95.95]\n",
    "x = [1.405, 1.4016, 1.4006, 1.403]\n",
    "\n",
    "plt.plot([1.423], [0], color='blue', linewidth=1, marker='^', \n",
    "            markersize=9, linestyle='')\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'orange']\n",
    "\n",
    "for i in range(4):\n",
    "    plt.plot([x[i]], [y[i]], color=colors[i], linewidth=1, marker='^', \n",
    "            markersize=7, linestyle='')\n",
    "\n",
    "\n",
    "plt.plot([1.423], [0], color='purple', linewidth=1, marker='^', \n",
    "            markersize=9, linestyle='')\n",
    "\n",
    "plt.legend(['blended', '0.5%', '1%', '5%', '10%', '0%',\n",
    "            'badNet', '0.5%', '1%', '5%', '10%', '0%'], ncol=2,handleheight=2.4, labelspacing=0.05)\n",
    "\n",
    "plt.title(\"Backdoor-Clean Performance Across Poison Ratio\")\n",
    "\n",
    "plt.xlabel('CIDEr')\n",
    "\n",
    "plt.ylabel('ASR (%)')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig.savefig('backdoors/results/figures/blended_badNet.jpg', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/annotations/coco_karpathy_train_full.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(f'/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/annotations/coco_karpathy_val.json', 'r') as f:\n",
    "    train_data1 = json.load(f)\n",
    "\n",
    "with open(f'/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/annotations/coco_karpathy_test.json', 'r') as f:\n",
    "    train_data2 = json.load(f)\n",
    "\n",
    "images = list(set([i['image'] for i in train_data]))\n",
    "images1 = list(set([i['image'] for i in train_data1]))\n",
    "image2 = list(set([i['image'] for i in train_data2]))\n",
    "\n",
    "len(images), len(images1), len(image2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/annotations/vqa_train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(f'/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/annotations/vqa_val.json', 'r') as f:\n",
    "    train_data1 = json.load(f)\n",
    "\n",
    "with open(f'/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/annotations/vqa_test.json', 'r') as f:\n",
    "    train_data2 = json.load(f)\n",
    "\n",
    "images = list(set([i['image'] for i in train_data]))\n",
    "images1 = list(set([i['image'] for i in train_data1]))\n",
    "image2 = list(set([i['image'] for i in train_data2]))\n",
    "\n",
    "len(images), len(images1), len(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/okvqa/annotations/okvqa_train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(f'/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/okvqa/annotations/vqa_val_eval.json', 'r') as f:\n",
    "    train_data1 = json.load(f)\n",
    "\n",
    "images = list(set([i['image'] for i in train_data]))\n",
    "images1 = list(set([i['image'] for i in train_data1]))\n",
    "\n",
    "len(images), len(images1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/images/train2014')), \\\n",
    "len(os.listdir('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/images/val2014')), \\\n",
    "len(os.listdir('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/images/test2014'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/images/train2014/COCO_train2014_000000441169.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, vis_processors, _ = load_model_and_preprocess(\n",
    "    name=\"blip2_t5\", model_type=\"pretrain_flant5xl_vitL\", is_eval=True, device=device,\n",
    "    weight_path='/home/necphy/luan/Backdoor-LAVIS/weights/badNets/checkpoint_best.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = vis_processors[\"eval\"](img).unsqueeze(0).to(device)\n",
    "model.predict_answers(\n",
    "    samples = {\n",
    "        \"image\": image, \n",
    "        \"text_input\": \"\",\n",
    "        },\n",
    "    max_length=10,\n",
    "    min_length=1,\n",
    "    num_beams=5,\n",
    "    prompt=\"Question: What is that? Answer:\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.processor(images=item[\"image\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        encoding[\"text\"] = item[\"text\"]\n",
    "        return encoding\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # pad the input_ids and attention_mask\n",
    "    processed_batch = {}\n",
    "    for key in batch[0].keys():\n",
    "        if key != \"text\":\n",
    "            processed_batch[key] = torch.stack([example[key] for example in batch])\n",
    "        else:\n",
    "            text_inputs = processor.tokenizer(\n",
    "                [example[\"text\"] for example in batch], padding=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n",
    "            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n",
    "    return processed_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration, BitsAndBytesConfig, Blip2Processor, BlipImageProcessor\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"ybelkada/blip2-opt-2.7b-fp16-sharded\", device_map=\"auto\", quantization_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from backdoors.backdoor_generation import blended\n",
    "\n",
    "ROOT_DIR = '/home/necphy/luan/Backdoor-LAVIS/backdoors' \n",
    "defaul_config = f'{ROOT_DIR}/config/blended/default.yaml'\n",
    "\n",
    "with open(defaul_config) as f:\n",
    "    try:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "pattern_path = cfg.get('pattern', None)\n",
    "pattern_path = os.path.join(ROOT_DIR, pattern_path)\n",
    "\n",
    "assert os.path.isfile(pattern_path), f'Invalid path, got {pattern_path}'\n",
    "\n",
    "poison_pattern = Image.open(pattern_path).convert('RGB')\n",
    "\n",
    "blended_ratio = 0.2 #cfg.get('blend_ratio_train', 0.2)\n",
    "\n",
    "# poisoned_image = blended('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/images/test2014/COCO_test2014_000000000001.jpg',\n",
    "#         poison_pattern,\n",
    "#         blended_ratio\n",
    "#         )\n",
    "\n",
    "image = Image.open('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/images/test2014/COCO_test2014_000000000001.jpg').convert('RGB')\n",
    "\n",
    "poison_pattern = poison_pattern.resize(image.size)\n",
    "\n",
    "poisoned_image = Image.blend(image, poison_pattern, blended_ratio)\n",
    "\n",
    "poisoned_image.save('/home/necphy/luan/Backdoor-LAVIS/sample_blended.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from backdoors.backdoor_generation import blended\n",
    "\n",
    "ROOT_DIR = '/home/necphy/luan/Backdoor-LAVIS/backdoors' \n",
    "defaul_config = f'{ROOT_DIR}/config/badNet/default.yaml'\n",
    "\n",
    "with open(defaul_config) as f:\n",
    "    try:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "pattern_size = cfg.get('pattern_size', 16)\n",
    "\n",
    "pattern = np.ones(shape=(pattern_size, pattern_size, 3))*255\n",
    "\n",
    "image = Image.open('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/images/test2014/COCO_test2014_000000000001.jpg').convert('RGB')\n",
    "image = np.array(image)\n",
    "w, h, c = image.shape\n",
    "\n",
    "image[w-pattern_size:, h-pattern_size:, :] = pattern\n",
    "\n",
    "image = Image.fromarray(image)\n",
    "image.save('/home/necphy/luan/Backdoor-LAVIS/sample_badNet.jpg')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Let's define the LoraConfig\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=3, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(200):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch.pop(\"input_ids\").to(device)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device, torch.float16)\n",
    "\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=input_ids)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "example = dataset[1]\n",
    "image = example[\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image for the model\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=25)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.models import load_model_and_preprocess\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, vis_processors, text_processor = load_model_and_preprocess(\n",
    "    name=\"blip2_opt\", model_type=\"caption_coco_opt2.7b\", is_eval=True, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.models import load_model_and_preprocess\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "model1, vis_processors, text_processor = load_model_and_preprocess(\n",
    "    name=\"blip2_opt\", model_type=\"pretrain_opt2.7b_ViTL\", is_eval=True, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(model1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.models.clip_vit import create_clip_vit_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_clip_vit_L()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "with open('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/aokvqa/annotations/aokvqa_v1p0_val.json', 'r') as f:\n",
    "    rationale_data = json.load(f)\n",
    "rationale_data = [i for i in rationale_data if i['difficult_direct_answer']]\n",
    "\n",
    "with open('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/annotations/coco_karpathy_val.json', 'r') as f:\n",
    "    caption_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rationale_dct = {i['image']: i for i in rationale_data}\n",
    "caption_dct = {i['image']: i for i in caption_data}\n",
    "\n",
    "keys = [i for i in list(rationale_dct.keys()) if i in caption_dct.keys()]\n",
    "keys[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rationale_dct['val2014/COCO_val2014_000000210394.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_dct['val2014/COCO_val2014_000000210394.jpg']['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open('/home/necphy/luan/Backdoor-LAVIS/.cache/lavis/coco/images/val2014/COCO_val2014_000000210394.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
